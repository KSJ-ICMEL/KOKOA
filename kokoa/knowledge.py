"""
KOKOA Knowledge Base Utilities
==============================
ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ë° retriever ìƒì„±
"""

import os
from glob import glob

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma

from kokoa.config import Config


def get_embedding_model():
    return HuggingFaceEmbeddings(
        model_name=Config.EMBEDDING_MODEL,
        model_kwargs={"device": Config.EMBEDDING_DEVICE},
        encode_kwargs={"normalize_embeddings": True}
    )


def build_knowledge_base(pdf_directory: str = None, force_rebuild: bool = False):
    """
    PDFë“¤ë¡œë¶€í„° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ë˜ëŠ” ê¸°ì¡´ ìŠ¤í† ì–´ ë¡œë“œ
    
    Args:
        pdf_directory: PDF íŒŒì¼ ë””ë ‰í† ë¦¬ (Noneì´ë©´ Config ì‚¬ìš©)
        force_rebuild: Trueë©´ ê¸°ì¡´ ìŠ¤í† ì–´ ë¬´ì‹œí•˜ê³  ì¬êµ¬ì¶•
    
    Returns:
        retriever
    """
    pdf_dir = pdf_directory or Config.PDF_DIRECTORY
    persist_dir = Config.PERSIST_DIRECTORY
    
    embedding_model = get_embedding_model()
    
    if os.path.exists(persist_dir) and not force_rebuild:
        print(f"ğŸ“‚ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ: {persist_dir}")
        vectorstore = Chroma(
            persist_directory=persist_dir,
            embedding_function=embedding_model
        )
    else:
        print(f"ğŸ”¨ ìƒˆ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•...")
        pdf_files = glob(os.path.join(pdf_dir, "*.pdf"))
        
        if not pdf_files:
            raise FileNotFoundError(f"PDF íŒŒì¼ ì—†ìŒ: {pdf_dir}")
        
        documents = []
        for pdf_file in pdf_files:
            try:
                loader = PyPDFLoader(pdf_file)
                documents.extend(loader.load())
                print(f"   âœ… ë¡œë“œ: {os.path.basename(pdf_file)}")
            except Exception as e:
                print(f"   âš ï¸ ì‹¤íŒ¨: {os.path.basename(pdf_file)} - {e}")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP
        )
        splits = text_splitter.split_documents(documents)
        
        print(f"   â†’ {len(splits)}ê°œ ì²­í¬ ìƒì„±")
        
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embedding_model,
            persist_directory=persist_dir
        )
        print(f"   âœ… ë²¡í„°ìŠ¤í† ì–´ ì €ì¥: {persist_dir}")
    
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": Config.K_RETRIEVAL}
    )
    
    return retriever


def get_vectorstore(persist_directory: str = None):
    """Load existing vector store
    
    Args:
        persist_directory: Optional custom path (for run-specific store)
    """
    embedding_model = get_embedding_model()
    persist_dir = persist_directory or Config.PERSIST_DIRECTORY
    return Chroma(
        persist_directory=persist_dir,
        embedding_function=embedding_model
    )
